# PPO: Proximal Policy Optimization

## 文件介绍：

+ ### 连续动作 + 并行线程的是DPPO
+ ### 离散动作的是simply_PPO
+ ### 离散动作 + 并行线程的是discrete_DPPO

